# -*- coding: utf-8 -*-
"""Submission 1 : NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GGsPAnSl39AdWcnicupu5Ch5HJu0oZKV
"""

# Impor library yang diperlukan
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dropout, Dense
import matplotlib.pyplot as plt

# Untuk menampilkan dataframe
df = pd.read_csv('bbc-text.csv')
df

# Untuk menampilkan jumlah colomns dan data
df.info()

# Panggil fungsi head() pada dataframe untuk menampilkan 5 sampel teratas pada dataset
df.head()

# Melakukan hot encording karena label kita merupakan data kategorikal
category = pd.get_dummies(df.category)
category

# Menampilkan jumlah per kategori
print("1. Business\t\t", len(df[df.category == 'business']))
print("2. Entertainment\t", len(df[df.category == 'entertainment'])) 
print("3. Politics\t\t", len(df[df.category == 'politics'])) 
print("4. Sport\t\t", len(df[df.category == 'sport'])) 
print("5. Tech\t\t\t", len(df[df.category == 'tech']))

# Melakukan one-hot-encoding dan membuat dataframe baru
category = pd.get_dummies(df.category)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns=['category'])
new_df

# Agar dapat diproses oleh model, kita perlu mengubah nilai-nilai dari dataframe ke dalam tipe data numpy array menggunakan atribut values
text = new_df['text'].values
label = new_df[['business', 'entertainment', 'politics', 'sport', 'tech']].values

# Kemudian bagi data untuk training dan data untuk testing
from sklearn.model_selection import train_test_split
text_latih, text_test, label_latih, label_test = train_test_split(text, label, test_size=0.2)

# Kemudian kita ubah setiap kata pada dataset kita ke dalam bilangan numerik dengan fungsi Tokenizer
# Setelah tokenisasi selesai, kita perlu membuat mengonversi setiap sampel menjadi sequence
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(text_latih) 
tokenizer.fit_on_texts(text_test)
 
sekuens_latih = tokenizer.texts_to_sequences(text_latih)
sekuens_test = tokenizer.texts_to_sequences(text_test)
 
padded_latih = pad_sequences(sekuens_latih, maxlen=50, padding='post', truncating='post') 
padded_test = pad_sequences(sekuens_test, maxlen=50, padding='post', truncating='post')

# Untuk arsitektur model kita menggunakan layer Embedding dengan dimensi embedding sebesar 32, serta dimensi dari input sebesar nilai num_words pada objek tokenizer
# Activation function yang digunakan pada layer terakhir dipilih softmax karena activation tersebut umum dipakai untuk klasifikasi multi kelas seperti ini
from tensorflow.keras.models import Sequential
from keras.layers import Bidirectional
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=32),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(120, activation='relu'),
    tf.keras.layers.Dense(60, activation='relu'),
    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

# Menentukan optimizer dan loss function dari model. Untuk masalah klasifikasi multi kelas, menggunakan loss ‘categorical_crossentropy’
model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Mengimplementasikan callback
# Dan mengset bila val_accuracy mencapai 92% otomatis akan stop training
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.92):
      if(logs.get('accuracy')>0.92):
        print("\nval_accuracy telah mencapai >92%!")
      self.model.stop_training = True
callbacks = myCallback()

# Menggunakan 30 epoch. atau bebas bereksperimen dengan nilai yang lain
# Mulai melatih model kita dengan memanggil fungsi fit()
num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

#  Menguji akurasi prediksi model pada data uji
#  Karena kita dapat membuat plot dari akurasi dan loss model kita pada saat proses pelatihan
model.evaluate(padded_latih, label_latih)

# Membuat plot akurasi atau loss dari model. Kode di bawah menunjukkan bagaimana kita bisa membuat plot loss dan accuracy dari model
# Untuk membuat loss kita bisa memanggil fungsi history pada objek history
# Untuk membuat plot dari akurasi kita bisa memilih metrik accuracy pada fungsi history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(26)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, 'r', label='Training Accuracy')
plt.plot(epochs_range, val_acc, 'b', label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, 'r', label='Training Loss')
plt.plot(epochs_range, val_loss, 'b', label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()